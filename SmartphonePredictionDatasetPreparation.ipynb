{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-5fVgIJzRgI",
        "outputId": "23844d6a-ae25-4189-9858-d9eae51db711"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating new dataset 'smartphone_unified_dataset_v2.csv'...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Simulating Devices: 100%|██████████| 50/50 [00:05<00:00,  9.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✅ New, more aggressive dataset generated successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import os\n",
        "\n",
        "# --- Optimized Configuration (UPDATED) ---\n",
        "N_DEVICES = 50\n",
        "N_FAILING_DEVICES = 15 # (30% failure rate)\n",
        "TIME_START = datetime(2023, 1, 1)\n",
        "TIME_END = datetime(2023, 12, 31)\n",
        "SAMPLING_INTERVAL_HOURS = 6\n",
        "OUTPUT_CSV = 'smartphone_unified_dataset_v2.csv' # New filename\n",
        "\n",
        "# --- Function to generate data for a single device ---\n",
        "def generate_device_data(device_id, failure_info):\n",
        "    timestamps = pd.to_datetime(np.arange(TIME_START, TIME_END, timedelta(hours=SAMPLING_INTERVAL_HOURS)))\n",
        "    n_samples = len(timestamps)\n",
        "    df = pd.DataFrame({'timestamp': timestamps, 'device_id': device_id})\n",
        "\n",
        "    # --- Generate Healthy Features ---\n",
        "    df['battery_level'] = 100 - (np.sin(np.arange(n_samples) / 4 * np.pi) + 1) * 40 + np.random.normal(0, 2, n_samples)\n",
        "    df['cpu_usage_percent'] = np.random.uniform(5, 30, n_samples) + np.sin(np.arange(n_samples) / 50) * 10\n",
        "    df['memory_usage_percent'] = np.random.uniform(40, 60, n_samples)\n",
        "    df['storage_usage_percent'] = np.linspace(20, 80, n_samples)\n",
        "    df['app_crashes'] = np.random.poisson(0.05, n_samples)\n",
        "    df['network_signal_strength_dbm'] = np.random.uniform(-110, -80, n_samples)\n",
        "    df['screen_on_time_minutes'] = np.random.exponential(30, n_samples)\n",
        "    df['fast_charging_active'] = np.random.choice([0, 1], size=n_samples, p=[0.8, 0.2])\n",
        "    df['speaker_volume_percent'] = np.random.uniform(0, 100, n_samples)\n",
        "    df['ambient_temp_c'] = np.random.uniform(15, 35, n_samples) + np.sin(np.arange(n_samples) / 1460 * 2 * np.pi) * 5\n",
        "\n",
        "    df['failure_type'], df['days_until_failure'], df['is_failing_soon'] = 0, 999.0, 0\n",
        "\n",
        "    if failure_info['is_failing']:\n",
        "        failure_point = int(n_samples * failure_info['failure_point_ratio'])\n",
        "        degradation_period = int(n_samples * 0.20) # A slightly longer degradation period\n",
        "        degradation_start = failure_point - degradation_period\n",
        "        degradation_factor = np.linspace(0, 1, n_samples - degradation_start)\n",
        "\n",
        "        # --- More Aggressive Failure Signals ---\n",
        "        if failure_info['failure_type'] == 1: # Battery\n",
        "            df.loc[degradation_start:, 'battery_level'] *= (1 - degradation_factor * 0.8) # Drains to 20%\n",
        "            df.loc[degradation_start:, 'ambient_temp_c'] += degradation_factor * 15 # Gets hot\n",
        "        elif failure_info['failure_type'] == 2: # CPU\n",
        "            df.loc[degradation_start:, 'cpu_usage_percent'] += degradation_factor * 60 # Higher CPU load\n",
        "            df.loc[degradation_start:, 'ambient_temp_c'] += degradation_factor * 25 # Gets very hot\n",
        "        elif failure_info['failure_type'] == 3: # Memory\n",
        "            df.loc[degradation_start:, 'memory_usage_percent'] += degradation_factor * 45 # Higher memory load\n",
        "            df.loc[degradation_start:, 'app_crashes'] += np.random.poisson(1.5, len(degradation_factor)) # More crashes\n",
        "\n",
        "        for i in range(degradation_start, failure_point):\n",
        "            days_left = (df.loc[failure_point, 'timestamp'] - df.loc[i, 'timestamp']).total_seconds() / (3600 * 24)\n",
        "            df.loc[i, 'days_until_failure'] = days_left\n",
        "            df.loc[i, 'failure_type'] = failure_info['failure_type']\n",
        "            if days_left <= 7:\n",
        "                df.loc[i, 'is_failing_soon'] = 1\n",
        "\n",
        "    # Clip values\n",
        "    for col in ['battery_level', 'cpu_usage_percent', 'memory_usage_percent', 'storage_usage_percent']:\n",
        "        df[col] = df[col].clip(0, 100)\n",
        "\n",
        "    return df\n",
        "\n",
        "# --- Main Generation Loop ---\n",
        "# (Same as before, but ensure you delete the old file if it exists)\n",
        "OUTPUT_CSV = 'smartphone_unified_dataset_v2.csv'\n",
        "if os.path.exists(OUTPUT_CSV):\n",
        "    os.remove(OUTPUT_CSV)\n",
        "\n",
        "all_device_ids = [f\"device_{i:03d}\" for i in range(N_DEVICES)]\n",
        "failing_device_ids = random.sample(all_device_ids, N_FAILING_DEVICES)\n",
        "failure_types = [1, 2, 3] # 1:Battery, 2:CPU, 3:Memory\n",
        "failure_infos = {}\n",
        "for i, device_id in enumerate(failing_device_ids):\n",
        "    failure_infos[device_id] = {\n",
        "        \"is_failing\": True,\n",
        "        \"failure_type\": failure_types[i % len(failure_types)],\n",
        "        \"failure_point_ratio\": random.uniform(0.8, 0.95)\n",
        "    }\n",
        "\n",
        "print(f\"Generating new dataset '{OUTPUT_CSV}'...\")\n",
        "header_written = False\n",
        "for i in tqdm(range(N_DEVICES), desc=\"Simulating Devices\"):\n",
        "    device_id = f\"device_{i:03d}\"\n",
        "    info = failure_infos.get(device_id, {\"is_failing\": False})\n",
        "    device_df = generate_device_data(device_id, info)\n",
        "    device_df.to_csv(OUTPUT_CSV, mode='a', header=not header_written, index=False)\n",
        "    header_written = True\n",
        "\n",
        "print(f\"\\n✅ New, more aggressive dataset generated successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import joblib\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "INPUT_CSV = 'smartphone_unified_dataset_v2.csv' # Using the new, larger dataset\n",
        "SEQUENCE_TIMESTEPS = 56 # 14 days * 4 readings/day\n",
        "\n",
        "def prepare_large_dataset():\n",
        "    \"\"\"\n",
        "    Prepares the large, unified dataset for the multi-task LSTM model,\n",
        "    optimized for memory efficiency.\n",
        "    \"\"\"\n",
        "    print(\"--- Phase 2: Preparing Large Dataset for Multi-Task Model ---\")\n",
        "\n",
        "    # --- 1. Load Data (Memory Optimized) ---\n",
        "    print(f\"Loading '{INPUT_CSV}'...\")\n",
        "    try:\n",
        "        dtype_map = {\n",
        "            'battery_level': 'float32', 'cpu_usage_percent': 'float32',\n",
        "            'memory_usage_percent': 'float32', 'storage_usage_percent': 'float32',\n",
        "            'app_crashes': 'int8', 'network_signal_strength_dbm': 'float32',\n",
        "            'screen_on_time_minutes': 'float32', 'fast_charging_active': 'int8',\n",
        "            'speaker_volume_percent': 'float32', 'ambient_temp_c': 'float32',\n",
        "            'failure_type': 'int8', 'days_until_failure': 'float32', 'is_failing_soon': 'int8'\n",
        "        }\n",
        "        df = pd.read_csv(INPUT_CSV, dtype=dtype_map, parse_dates=['timestamp'])\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: The file '{INPUT_CSV}' was not found. Please ensure it was generated correctly.\")\n",
        "        return\n",
        "\n",
        "    df.sort_values(by=['device_id', 'timestamp'], inplace=True)\n",
        "    print(\"Data loaded successfully.\")\n",
        "    df.info(memory_usage='deep')\n",
        "\n",
        "    # --- 2. Scale Features ---\n",
        "    feature_columns = ['battery_level', 'cpu_usage_percent', 'memory_usage_percent', 'storage_usage_percent', 'app_crashes', 'network_signal_strength_dbm', 'screen_on_time_minutes', 'fast_charging_active', 'speaker_volume_percent', 'ambient_temp_c']\n",
        "\n",
        "    print(\"\\nScaling features...\")\n",
        "    scaler = MinMaxScaler()\n",
        "    df[feature_columns] = scaler.fit_transform(df[feature_columns])\n",
        "    joblib.dump(scaler, 'multi_task_scaler.joblib')\n",
        "    print(\"Scaler saved to 'multi_task_scaler.joblib'.\")\n",
        "\n",
        "    # --- 3. Create Sequences ---\n",
        "    print(f\"\\nCreating sequences of {SEQUENCE_TIMESTEPS} timesteps...\")\n",
        "    sequences, labels_why, labels_when, labels_if = [], [], [], []\n",
        "\n",
        "    # Group by device to ensure sequences don't cross over different devices\n",
        "    grouped_data = df.groupby('device_id')\n",
        "\n",
        "    for _, group in tqdm(grouped_data, desc=\"Creating Sequences\"):\n",
        "        features = group[feature_columns].values\n",
        "        label_why = group['failure_type'].values\n",
        "        label_when = group['days_until_failure'].values\n",
        "        label_if = group['is_failing_soon'].values\n",
        "\n",
        "        # Slide a window across each group's data\n",
        "        for i in range(len(features) - SEQUENCE_TIMESTEPS):\n",
        "            sequences.append(features[i:i + SEQUENCE_TIMESTEPS])\n",
        "            # The label corresponds to the END of the sequence window\n",
        "            labels_why.append(label_why[i + SEQUENCE_TIMESTEPS - 1])\n",
        "            labels_when.append(label_when[i + SEQUENCE_TIMESTEPS - 1])\n",
        "            labels_if.append(label_if[i + SEQUENCE_TIMESTEPS - 1])\n",
        "\n",
        "    X = np.array(sequences)\n",
        "    y_why = np.array(labels_why)\n",
        "    y_when = np.array(labels_when)\n",
        "    y_if = np.array(labels_if)\n",
        "\n",
        "    # --- 4. Save Processed Data ---\n",
        "    print(\"\\nSaving processed data to .npy files...\")\n",
        "    np.save('sequences_large.npy', X)\n",
        "    np.save('labels_why_large.npy', y_why)\n",
        "    np.save('labels_when_large.npy', y_when)\n",
        "    np.save('labels_if_large.npy', y_if)\n",
        "\n",
        "    print(\"\\n--- Data Preparation Summary ---\")\n",
        "    print(f\"Total sequences created: {X.shape[0]}\")\n",
        "    print(f\"Sequence shape (X): {X.shape}\")\n",
        "    print(f\"Labels 'Why' shape (y_why): {y_why.shape}\")\n",
        "    print(f\"Labels 'When' shape (y_when): {y_when.shape}\")\n",
        "    print(f\"Labels 'If' shape (y_if): {y_if.shape}\")\n",
        "    print(\"\\n✅ Data preparation complete. You are now ready to train the multi-task model.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare_large_dataset()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejGfynPS3AdQ",
        "outputId": "20ad672b-0315-4f0b-c783-808eb8477c43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Phase 2: Preparing Large Dataset for Multi-Task Model ---\n",
            "Loading 'smartphone_unified_dataset_v2.csv'...\n",
            "Data loaded successfully.\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 72800 entries, 0 to 72799\n",
            "Data columns (total 15 columns):\n",
            " #   Column                       Non-Null Count  Dtype         \n",
            "---  ------                       --------------  -----         \n",
            " 0   timestamp                    72800 non-null  datetime64[ns]\n",
            " 1   device_id                    72800 non-null  object        \n",
            " 2   battery_level                72800 non-null  float32       \n",
            " 3   cpu_usage_percent            72800 non-null  float32       \n",
            " 4   memory_usage_percent         72800 non-null  float32       \n",
            " 5   storage_usage_percent        72800 non-null  float32       \n",
            " 6   app_crashes                  72800 non-null  int8          \n",
            " 7   network_signal_strength_dbm  72800 non-null  float32       \n",
            " 8   screen_on_time_minutes       72800 non-null  float32       \n",
            " 9   fast_charging_active         72800 non-null  int8          \n",
            " 10  speaker_volume_percent       72800 non-null  float32       \n",
            " 11  ambient_temp_c               72800 non-null  float32       \n",
            " 12  failure_type                 72800 non-null  int8          \n",
            " 13  days_until_failure           72800 non-null  float32       \n",
            " 14  is_failing_soon              72800 non-null  int8          \n",
            "dtypes: datetime64[ns](1), float32(9), int8(4), object(1)\n",
            "memory usage: 7.4 MB\n",
            "\n",
            "Scaling features...\n",
            "Scaler saved to 'multi_task_scaler.joblib'.\n",
            "\n",
            "Creating sequences of 56 timesteps...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Creating Sequences: 100%|██████████| 50/50 [00:00<00:00, 340.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Saving processed data to .npy files...\n",
            "\n",
            "--- Data Preparation Summary ---\n",
            "Total sequences created: 70000\n",
            "Sequence shape (X): (70000, 56, 10)\n",
            "Labels 'Why' shape (y_why): (70000,)\n",
            "Labels 'When' shape (y_when): (70000,)\n",
            "Labels 'If' shape (y_if): (70000,)\n",
            "\n",
            "✅ Data preparation complete. You are now ready to train the multi-task model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dcxU9O0n3aOy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}